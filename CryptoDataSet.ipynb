{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'custom_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 102\u001b[0m\n\u001b[0;32m     98\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(current_batch_size)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Example usage with validation\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mgenerate_encryption_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcipher_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAES\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m, in \u001b[0;36mgenerate_encryption_dataset\u001b[1;34m(cipher_type, num_samples, batch_size)\u001b[0m\n\u001b[0;32m     39\u001b[0m     columns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnonce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Create initial Parquet file with metadata\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencryption_dataset.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Generate data in memory-efficient batches\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mnum_samples, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n",
      "File \u001b[1;32mc:\\Users\\Masam\\miniconda3\\envs\\tf\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Masam\\miniconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3034\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3114\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3115\u001b[0m     path,\n\u001b[0;32m   3116\u001b[0m     engine,\n\u001b[0;32m   3117\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3118\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3119\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3120\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3122\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Masam\\miniconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 480\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    481\u001b[0m     df,\n\u001b[0;32m    482\u001b[0m     path_or_buf,\n\u001b[0;32m    483\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    484\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    485\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m    486\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    487\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    489\u001b[0m )\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32mc:\\Users\\Masam\\miniconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parquet.py:228\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[0;32m    219\u001b[0m             table,\n\u001b[0;32m    220\u001b[0m             path_or_handle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    225\u001b[0m         )\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_table(\n\u001b[0;32m    229\u001b[0m             table,\n\u001b[0;32m    230\u001b[0m             path_or_handle,\n\u001b[0;32m    231\u001b[0m             compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    232\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    234\u001b[0m         )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Masam\\miniconda3\\envs\\tf\\lib\\site-packages\\pyarrow\\parquet\\core.py:1869\u001b[0m, in \u001b[0;36mwrite_table\u001b[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **kwargs)\u001b[0m\n\u001b[0;32m   1867\u001b[0m use_int96 \u001b[38;5;241m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1869\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ParquetWriter(\n\u001b[0;32m   1870\u001b[0m             where, table\u001b[38;5;241m.\u001b[39mschema,\n\u001b[0;32m   1871\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   1872\u001b[0m             version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[0;32m   1873\u001b[0m             flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[0;32m   1874\u001b[0m             use_dictionary\u001b[38;5;241m=\u001b[39muse_dictionary,\n\u001b[0;32m   1875\u001b[0m             write_statistics\u001b[38;5;241m=\u001b[39mwrite_statistics,\n\u001b[0;32m   1876\u001b[0m             coerce_timestamps\u001b[38;5;241m=\u001b[39mcoerce_timestamps,\n\u001b[0;32m   1877\u001b[0m             data_page_size\u001b[38;5;241m=\u001b[39mdata_page_size,\n\u001b[0;32m   1878\u001b[0m             allow_truncated_timestamps\u001b[38;5;241m=\u001b[39mallow_truncated_timestamps,\n\u001b[0;32m   1879\u001b[0m             compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1880\u001b[0m             use_deprecated_int96_timestamps\u001b[38;5;241m=\u001b[39muse_int96,\n\u001b[0;32m   1881\u001b[0m             compression_level\u001b[38;5;241m=\u001b[39mcompression_level,\n\u001b[0;32m   1882\u001b[0m             use_byte_stream_split\u001b[38;5;241m=\u001b[39muse_byte_stream_split,\n\u001b[0;32m   1883\u001b[0m             column_encoding\u001b[38;5;241m=\u001b[39mcolumn_encoding,\n\u001b[0;32m   1884\u001b[0m             data_page_version\u001b[38;5;241m=\u001b[39mdata_page_version,\n\u001b[0;32m   1885\u001b[0m             use_compliant_nested_type\u001b[38;5;241m=\u001b[39muse_compliant_nested_type,\n\u001b[0;32m   1886\u001b[0m             encryption_properties\u001b[38;5;241m=\u001b[39mencryption_properties,\n\u001b[0;32m   1887\u001b[0m             write_batch_size\u001b[38;5;241m=\u001b[39mwrite_batch_size,\n\u001b[0;32m   1888\u001b[0m             dictionary_pagesize_limit\u001b[38;5;241m=\u001b[39mdictionary_pagesize_limit,\n\u001b[0;32m   1889\u001b[0m             store_schema\u001b[38;5;241m=\u001b[39mstore_schema,\n\u001b[0;32m   1890\u001b[0m             write_page_index\u001b[38;5;241m=\u001b[39mwrite_page_index,\n\u001b[0;32m   1891\u001b[0m             write_page_checksum\u001b[38;5;241m=\u001b[39mwrite_page_checksum,\n\u001b[0;32m   1892\u001b[0m             sorting_columns\u001b[38;5;241m=\u001b[39msorting_columns,\n\u001b[0;32m   1893\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m   1894\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Masam\\miniconda3\\envs\\tf\\lib\\site-packages\\pyarrow\\parquet\\core.py:1002\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[1;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **options)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collector \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_collector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1001\u001b[0m engine_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1002\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m _parquet\u001b[38;5;241m.\u001b[39mParquetWriter(\n\u001b[0;32m   1003\u001b[0m     sink, schema,\n\u001b[0;32m   1004\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[0;32m   1005\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1006\u001b[0m     use_dictionary\u001b[38;5;241m=\u001b[39muse_dictionary,\n\u001b[0;32m   1007\u001b[0m     write_statistics\u001b[38;5;241m=\u001b[39mwrite_statistics,\n\u001b[0;32m   1008\u001b[0m     use_deprecated_int96_timestamps\u001b[38;5;241m=\u001b[39muse_deprecated_int96_timestamps,\n\u001b[0;32m   1009\u001b[0m     compression_level\u001b[38;5;241m=\u001b[39mcompression_level,\n\u001b[0;32m   1010\u001b[0m     use_byte_stream_split\u001b[38;5;241m=\u001b[39muse_byte_stream_split,\n\u001b[0;32m   1011\u001b[0m     column_encoding\u001b[38;5;241m=\u001b[39mcolumn_encoding,\n\u001b[0;32m   1012\u001b[0m     writer_engine_version\u001b[38;5;241m=\u001b[39mengine_version,\n\u001b[0;32m   1013\u001b[0m     data_page_version\u001b[38;5;241m=\u001b[39mdata_page_version,\n\u001b[0;32m   1014\u001b[0m     use_compliant_nested_type\u001b[38;5;241m=\u001b[39muse_compliant_nested_type,\n\u001b[0;32m   1015\u001b[0m     encryption_properties\u001b[38;5;241m=\u001b[39mencryption_properties,\n\u001b[0;32m   1016\u001b[0m     write_batch_size\u001b[38;5;241m=\u001b[39mwrite_batch_size,\n\u001b[0;32m   1017\u001b[0m     dictionary_pagesize_limit\u001b[38;5;241m=\u001b[39mdictionary_pagesize_limit,\n\u001b[0;32m   1018\u001b[0m     store_schema\u001b[38;5;241m=\u001b[39mstore_schema,\n\u001b[0;32m   1019\u001b[0m     write_page_index\u001b[38;5;241m=\u001b[39mwrite_page_index,\n\u001b[0;32m   1020\u001b[0m     write_page_checksum\u001b[38;5;241m=\u001b[39mwrite_page_checksum,\n\u001b[0;32m   1021\u001b[0m     sorting_columns\u001b[38;5;241m=\u001b[39msorting_columns,\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Masam\\miniconda3\\envs\\tf\\lib\\site-packages\\pyarrow\\_parquet.pyx:2101\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'custom_metadata'"
     ]
    }
   ],
   "source": [
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "def generate_encryption_dataset(cipher_type='AES', num_samples=10_000_000, batch_size=100_000):\n",
    "    \"\"\"\n",
    "    Generates a large-scale encryption dataset with cryptographic integrity checks\n",
    "    \n",
    "    Args:\n",
    "        cipher_type: 'AES' or 'ChaCha20'\n",
    "        num_samples: Total number of samples to generate\n",
    "        batch_size: Number of samples per write batch\n",
    "    \n",
    "    Output:\n",
    "        encryption_dataset.parquet with schema:\n",
    "        - plaintext: bytes (16-256 bytes)\n",
    "        - ciphertext: bytes\n",
    "        - key: bytes\n",
    "        - nonce: bytes (ChaCha20 only)\n",
    "    \"\"\"\n",
    "    # Dataset metadata\n",
    "    metadata = {\n",
    "        'algorithm': cipher_type,\n",
    "        'creation_date': datetime.utcnow().isoformat(),\n",
    "        'total_samples': str(num_samples),\n",
    "        'version': '1.1',\n",
    "        'author': 'Genetic Programming Cryptanalysis Suite',\n",
    "        'security_note': 'FOR RESEARCH USE ONLY - UNSAFE FOR PRODUCTION'\n",
    "    }\n",
    "\n",
    "    # Initialize schema based on cipher type\n",
    "    columns = ['plaintext', 'ciphertext', 'key']\n",
    "    if cipher_type == 'ChaCha20':\n",
    "        columns.append('nonce')\n",
    "\n",
    "    # Create initial Parquet file with metadata\n",
    "    pd.DataFrame(columns=columns).to_parquet(\n",
    "        'encryption_dataset.parquet',\n",
    "        engine='pyarrow',\n",
    "        compression=None,\n",
    "        custom_metadata=metadata\n",
    "    )\n",
    "\n",
    "    # Generate data in memory-efficient batches\n",
    "    with tqdm(total=num_samples, desc='Generating Dataset') as pbar:\n",
    "        for batch_idx in range(0, num_samples, batch_size):\n",
    "            current_batch_size = min(batch_size, num_samples - batch_idx)\n",
    "            batch_data = {col: [] for col in columns}\n",
    "            \n",
    "            for _ in range(current_batch_size):\n",
    "                # Generate random plaintext (16-256 bytes)\n",
    "                pt_length = random.randint(16, 256)\n",
    "                pt = os.urandom(pt_length)\n",
    "                key = os.urandom(16 if cipher_type == 'AES' else 8)\n",
    "                \n",
    "                # Encrypt with selected algorithm\n",
    "                if cipher_type == 'AES':\n",
    "                    cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())\n",
    "                    nonce = None\n",
    "                elif cipher_type == 'ChaCha20':\n",
    "                    nonce = os.urandom(16)\n",
    "                    cipher = Cipher(algorithms.ChaCha20(key, nonce), mode=None, backend=default_backend())\n",
    "                \n",
    "                encryptor = cipher.encryptor()\n",
    "                ct = encryptor.update(pt) + encryptor.finalize()\n",
    "                \n",
    "                # Store results\n",
    "                batch_data['plaintext'].append(pt)\n",
    "                batch_data['ciphertext'].append(ct)\n",
    "                batch_data['key'].append(key)\n",
    "                if cipher_type == 'ChaCha20':\n",
    "                    batch_data['nonce'].append(nce)\n",
    "            \n",
    "            # Create batch dataframe\n",
    "            df = pd.DataFrame(batch_data)\n",
    "            \n",
    "            # Add batch integrity check\n",
    "            batch_hash = hashlib.sha3_256(\n",
    "                b''.join(df['ciphertext'] + df['key'])\n",
    "            ).hexdigest()\n",
    "            df.attrs['batch_hash'] = batch_hash\n",
    "            \n",
    "            # Append to Parquet file\n",
    "            df.to_parquet(\n",
    "                'encryption_dataset.parquet',\n",
    "                engine='pyarrow',\n",
    "                compression='zstd',\n",
    "                index=False,\n",
    "                append=True,\n",
    "                existing_metadata='update'\n",
    "            )\n",
    "            \n",
    "            pbar.update(current_batch_size)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage with validation\n",
    "    generate_encryption_dataset(\n",
    "        cipher_type='AES',\n",
    "        num_samples=10_000_000,\n",
    "        batch_size=100_000\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
